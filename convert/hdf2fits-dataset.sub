##############################################################
# HTCondor submit description file template to process the
# conversion of an HDF dataset into FITS format.
#
# The file sequence at the "queue" command below
# represents the input data files to be processed
# as individual jobs.
# The name of the dataset to be converted is specified
# via the DATASET keyword below.
# If needed the dataset name should also include the
# group specification to which the dataset belongs.
#
# From each input file the filename base is constructed
# in the actual processing shell script (see below) and
# the extension .fits will be added for the output file.
#
# For each individual job also a jobname is constructed
# to identify individual jobs on the batch system.
# This jobname is also used in the naming of the log files,
# together with the Cluster and Process name. 
#
# The actual shell script for processing is contained
# in the file hdf2fits-dataset.sh, which in turn invokes
# the file hdf2fits-dataset.py containing the python script.
#
# In order to launch the batch execution, the files
# hdf2fits-dataset.sh and hdf2fits-dataset.py have to reside
# in the directory from which this script is invoked.
#
# To launch the job(s) issue the command ($ = prompt) :
#
# $ condor_submit hdf2fits-dataset.sub
#
# All log files will be returned into the (working aka host)
# directory from which the job was launched.
# 
# Nick van Eijndhoven, IIHE-VUB, Brussels.
# UTC February 14, 2022  16:23
##############################################################

universe   = vanilla
getenv     = TRUE
HOMEDIR    = $ENV(HOME)
HOSTDIR    = $ENV(PWD)

OUTPUTDIR  = $(HOSTDIR)

DATASET    = MCMuon

# Enable the HTCondor file transfer mechanism
should_transfer_files = YES

# Suppress the automatic transfer of output files (except for log files)
transfer_output_files=""

executable = hdf2fits-dataset.sh

batch_name = hdf2fits-dataset

JOBNAME    = $(batch_name)$(Process)

arguments  = $(JOBNAME) $(HOMEDIR) $(HOSTDIR) $(OUTPUTDIR) $(filename) $(DATASET)

joblog     = $(JOBNAME)-$(Cluster)-$(Process)-job.log
sublog     = $(JOBNAME)-$(Cluster)-$(Process)-sub.log

error      = $(joblog)
output     = $(joblog)
log        = $(sublog)

queue filename matching files $(HOSTDIR)/Gen2*.hdf5

